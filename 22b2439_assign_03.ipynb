{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "'''\n",
    "Using pandas load the imdb reviews csv file\n",
    "Analyse its shape and columns\n",
    "Check for missing data and remove those rows\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "imdb = pd.read_csv(r\"C:\\Users\\ajinkya\\Downloads\\IMDB_Dataset.csv\")\n",
    "print(\"dataset :\\n\",imdb)\n",
    "print(\"Shape of dataset : \",imdb.shape)\n",
    "print(\"columns in dataset : \",imdb.columns)\n",
    "\n",
    "missing_data = imdb.isnull().sum()\n",
    "print(\"missing data : \\n\",missing_data)\n",
    "\n",
    "imdb_cleaned = imdb.dropna()\n",
    "print(\"Cleaned dataset :\",imdb_cleaned)\n",
    "print(\"Cleaned shape : \",imdb_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Complete the function to preprocess the text data\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    imdb = sentence\n",
    "    # First make the sentence lowercase\n",
    "    imdb = imdb.lower()\n",
    "    \n",
    "    \n",
    "    # Remove all html tags from the sentence i.e replace anything between <> with space\n",
    "    # Hint use Regular Expression i.e. re.sub()\n",
    "    imdb = re.sub(r'<.*?>',' ',imdb)\n",
    "    \n",
    "    \n",
    "    # Remove all special characters i.e. anything other than alphabets and numbers. Replace them with space\n",
    "    imdb = re.sub(r'[^a-zA-Z0-9\\s]', ' ' , imdb)\n",
    "    \n",
    "    # Remove all single characters i.e. a-z and A-Z and Replace them with space\n",
    "    imdb = re.sub(r'\\b[a-zA-Z]\\b', ' ', imdb)\n",
    "    \n",
    "    # Remove all multiple spaces and replace them with single space\n",
    "    imdb = re.sub(r'\\s+', ' ', imdb)\n",
    "    \n",
    "    \n",
    "    # Use the nltk library to remove all stopwords from the sentence\n",
    "    # stopwords are the words like and, the, is, are etc.\n",
    "    import nltk \n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = imdb.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    imdb = ' '.join(filtered_words)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # return the sentence\n",
    "    return imdb\n",
    "     # remove this line after writing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :\n",
    "# Call the preprocessing function for each review in the dataframe and\n",
    "# save the results in a new list of preprocessed_reviews\n",
    "# This list will be your input to the neural network\n",
    "# We will call this list as X from now on\n",
    "preprocessed_series = imdb['review'].apply(preprocessing) \n",
    "X = preprocessed_series.tolist()\n",
    "\n",
    "#for review in X:\n",
    "#    print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :\n",
    "# Convert sentiment column in the dataframe to numbers\n",
    "# Convert positive to 1 and negative to 0 and store it in numpy array\n",
    "# We will call this numpy array as y from now on\n",
    "\n",
    "imdb['sentiment'] = imdb['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "print(imdb['sentiment'])\n",
    "#convert into numpy array\n",
    "y = imdb['sentiment'].to_numpy()\n",
    "\n",
    "\n",
    "#print(y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a TODO : Split the data into training and testing (80-20 ratio)\n",
    "# The train set will be used to train our deep learning models \n",
    "# while test set will be used to evaluate how well our model performs \n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing embedding layer\n",
    "Let's now write the script for our embedding layer. Embedding layer converts our textual data into numeric form. It is then **used as the first layer for the deep learning model like LSTM**.  \n",
    "To know more about word embedding you may refer to following video\n",
    "https://www.youtube.com/watch?v=9S0-OC4LFNo  \n",
    "#### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92308"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "word_tokenizer = Tokenizer()\n",
    "\n",
    "# TODO: Fit the tokenizer on the training data (X_train)\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# TODO: Convert training data to sequences of integers\n",
    "# Hint: Use texts_to_sequences method\n",
    "X_train_sequences = word_tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# TODO: Convert test data to sequences of integers\n",
    "# Hint: Use texts_to_sequences method\n",
    "X_test_sequences = word_tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# End TODO\n",
    "# Saving the tokenizer in a json file (Already done for you)\n",
    "# This will be used later for prediction on data in next assignments\n",
    "tokenizer_json = word_tokenizer.to_json()\n",
    "with io.open('b3_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    \n",
    "# Vocab_length is the number of unique words in our dataset\n",
    "# Adding 1 to store dimensions for words for which no pretrained word embeddings exist\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding all reviews to be of same length 'maxlen' words\n",
    "maxlen = 100\n",
    "# You can try different dimensions like 50, 100, 200 and 300\n",
    "# and see how the model performs in next week\n",
    "\n",
    "# TODO: Pad the training data sequences\n",
    "# Hint: Use pad_sequences with 'post' padding and maxlen=maxlen\n",
    "X_train_padded = pad_sequences(X_train_sequences, padding = 'post', maxlen = maxlen)\n",
    "\n",
    "# TODO: Pad the test data sequences\n",
    "# Hint: Use pad_sequences with 'post' padding and maxlen=maxlen\n",
    "X_test_padded = pad_sequences(X_test_sequences, padding='post', maxlen = maxlen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary for embeddings\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "# Open the GloVe file (a2_glove.6B.100d.txt) with utf-8 encoding\n",
    "glove_file = open(r'C:\\Users\\ajinkya\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\Codes_all\\SOC_codes\\codes\\glove_embeddings.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "# TODO : Create an embedding matrix where each row corresponds to the index of the\n",
    "# unique word in the dataset and each column corresponds to the word vector\n",
    "# in the GloVe embedding \n",
    "# So the matrix will have vocab_length rows and maxlen columns\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment_04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Create a model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length, output_dim=embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO 3: Train the model\n",
    "# Assuming y_train and y_test are defined and contain the target values\n",
    "history = model.fit(X_train_padded, Y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "# TODO 4: Evaluate the model with the test data\n",
    "loss, accuracy = model.evaluate(X_test_padded, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: Print the accuracy of the model\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "# Optional: Plot training and validation accuracy and loss\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# TODO 6: Save the model\n",
    "model.save('lstm_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "### 4_b\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# TODO 7: Load the model that you saved in the previous cell using load_model() function.\n",
    "model = load_model(r'C:\\Users\\ajinkya\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\Codes_all\\SOC_codes\\codes\\lstm_model.h5')\n",
    "\n",
    "# TODO 8: Load the IMDB Dataset using pandas\n",
    "# Assuming the dataset is stored in 'imdb_reviews.csv'\n",
    "imdb_df = pd.read_csv(r'C:\\Users\\ajinkya\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\Codes_all\\SOC_codes\\Data\\IMDB_Dataset.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(imdb_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 9: Preprocess the data as you did in the previous assignment.\n",
    "# Let's assume the review text is in a column named 'review'\n",
    "reviews = imdb_df['review'].values\n",
    "\n",
    "# Load the tokenizer from the json file\n",
    "with open('b3_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step\n"
     ]
    }
   ],
   "source": [
    "# TODO 10: Tokenize the data using the tokenizer_from_json() function\n",
    "review_sequences = tokenizer.texts_to_sequences(reviews)\n",
    "\n",
    "# TODO 11: Pad the data using the pad_sequences() function\n",
    "review_padded = pad_sequences(review_sequences, padding='post', maxlen=100)\n",
    "\n",
    "# TODO 12: Make predictions on the data using the model.predict() function\n",
    "predictions = model.predict(review_padded)\n",
    "\n",
    "# TODO 13: For each review, if the prediction is stored in variable, 'prediction',\n",
    "# then np.round(prediction*10,1) will give you the predicted rating\n",
    "predicted_ratings = np.round(predictions * 10, 1)\n",
    "\n",
    "# Add the predicted ratings to the dataframe\n",
    "imdb_df['predicted_rating'] = predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...  positive   \n",
      "1  A wonderful little production. <br /><br />The...  positive   \n",
      "2  I thought this was a wonderful way to spend ti...  positive   \n",
      "3  Basically there's a family where a little boy ...  negative   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "\n",
      "   predicted_rating  \n",
      "0               6.2  \n",
      "1               9.8  \n",
      "2               9.9  \n",
      "3               1.5  \n",
      "4               9.7  \n"
     ]
    }
   ],
   "source": [
    "# Save the results back to a csv file for comparison\n",
    "imdb_df.to_csv('imdb_reviews_with_predictions.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "print(imdb_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

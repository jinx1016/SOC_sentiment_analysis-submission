{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Create a model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length, output_dim=embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Optional: Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: Train the model\n",
    "# Assuming y_train and y_test are defined and contain the target values\n",
    "history = model.fit(X_train_padded, y_train, batch_size=32, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: Evaluate the model with the test data\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: Print the accuracy of the model\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Plot training and validation accuracy and loss\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6: Save the model\n",
    "model.save('lstm_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# TODO 7: Load the model that you saved in the previous cell using load_model() function.\n",
    "model = load_model(r'C:\\Users\\ajinkya\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\Codes_all\\SOC_codes\\codes\\lstm_model.h5')\n",
    "\n",
    "# TODO 8: Load the IMDB Dataset using pandas\n",
    "# Assuming the dataset is stored in 'imdb_reviews.csv'\n",
    "imdb_df = pd.read_csv(r'C:\\Users\\ajinkya\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\Codes_all\\SOC_codes\\Data\\IMDB_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the dataframe\n",
    "print(imdb_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 9: Preprocess the data as you did in the previous assignment.\n",
    "# Let's assume the review text is in a column named 'review'\n",
    "reviews = imdb_df['review'].values\n",
    "\n",
    "# Load the tokenizer from the json file\n",
    "with open('b3_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step\n"
     ]
    }
   ],
   "source": [
    "# TODO 10: Tokenize the data using the tokenizer_from_json() function\n",
    "review_sequences = tokenizer.texts_to_sequences(reviews)\n",
    "\n",
    "# TODO 11: Pad the data using the pad_sequences() function\n",
    "review_padded = pad_sequences(review_sequences, padding='post', maxlen=100)\n",
    "\n",
    "# TODO 12: Make predictions on the data using the model.predict() function\n",
    "predictions = model.predict(review_padded)\n",
    "\n",
    "# TODO 13: For each review, if the prediction is stored in variable, 'prediction',\n",
    "# then np.round(prediction*10,1) will give you the predicted rating\n",
    "predicted_ratings = np.round(predictions * 10, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...  positive   \n",
      "1  A wonderful little production. <br /><br />The...  positive   \n",
      "2  I thought this was a wonderful way to spend ti...  positive   \n",
      "3  Basically there's a family where a little boy ...  negative   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "\n",
      "   predicted_rating  \n",
      "0               6.2  \n",
      "1               9.8  \n",
      "2               9.9  \n",
      "3               1.5  \n",
      "4               9.7  \n"
     ]
    }
   ],
   "source": [
    "# Add the predicted ratings to the dataframe\n",
    "imdb_df['predicted_rating'] = predicted_ratings\n",
    "\n",
    "# Save the results back to a csv file for comparison\n",
    "imdb_df.to_csv('imdb_reviews_with_predictions.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "print(imdb_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
